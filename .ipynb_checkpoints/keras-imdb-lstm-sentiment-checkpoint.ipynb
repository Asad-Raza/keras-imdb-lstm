{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "'''Trains an LSTM model on the IMDB sentiment classification task.\n",
    "\n",
    "The dataset is actually too small for LSTM to be of any advantage\n",
    "compared to simpler, much faster methods such as TF-IDF + LogReg.\n",
    "\n",
    "# Notes\n",
    "\n",
    "- RNNs are tricky. Choice of batch size is important,\n",
    "choice of loss and optimizer is critical, etc.\n",
    "Some configurations won't converge.\n",
    "\n",
    "- LSTM loss decrease patterns during training can be quite different\n",
    "from what you see with CNNs/MLPs/etc.\n",
    "'''\n",
    "from __future__ import print_function\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.datasets import imdb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 20000\n",
    "# cut texts after this number of words (among top max_features most common words)\n",
    "maxlen = 170 #80\n",
    "batch_size = 32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "25000 train sequences\n",
      "25000 test sequences\n"
     ]
    }
   ],
   "source": [
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list([1, 591, 202, 14, 31, 6, 717, 10, 10, 18142, 10698, 5, 4, 360, 7, 4, 177, 5760, 394, 354, 4, 123, 9, 1035, 1035, 1035, 10, 10, 13, 92, 124, 89, 488, 7944, 100, 28, 1668, 14, 31, 23, 27, 7479, 29, 220, 468, 8, 124, 14, 286, 170, 8, 157, 46, 5, 27, 239, 16, 179, 15387, 38, 32, 25, 7944, 451, 202, 14, 6, 717]),\n",
       "       list([1, 14, 22, 3443, 6, 176, 7, 5063, 88, 12, 2679, 23, 1310, 5, 109, 943, 4, 114, 9, 55, 606, 5, 111, 7, 4, 139, 193, 273, 23, 4, 172, 270, 11, 7216, 10626, 4, 8463, 2801, 109, 1603, 21, 4, 22, 3861, 8, 6, 1193, 1330, 10, 10, 4, 105, 987, 35, 841, 16873, 19, 861, 1074, 5, 1987, 17975, 45, 55, 221, 15, 670, 5304, 526, 14, 1069, 4, 405, 5, 2438, 7, 27, 85, 108, 131, 4, 5045, 5304, 3884, 405, 9, 3523, 133, 5, 50, 13, 104, 51, 66, 166, 14, 22, 157, 9, 4, 530, 239, 34, 8463, 2801, 45, 407, 31, 7, 41, 3778, 105, 21, 59, 299, 12, 38, 950, 5, 4521, 15, 45, 629, 488, 2733, 127, 6, 52, 292, 17, 4, 6936, 185, 132, 1988, 5304, 1799, 488, 2693, 47, 6, 392, 173, 4, 2, 4378, 270, 2352, 4, 1500, 7, 4, 65, 55, 73, 11, 346, 14, 20, 9, 6, 976, 2078, 7, 5293, 861, 12746, 5, 4182, 30, 3127, 2, 56, 4, 841, 5, 990, 692, 8, 4, 1669, 398, 229, 10, 10, 13, 2822, 670, 5304, 14, 9, 31, 7, 27, 111, 108, 15, 2033, 19, 7836, 1429, 875, 551, 14, 22, 9, 1193, 21, 45, 4829, 5, 45, 252, 8, 12508, 6, 565, 921, 3639, 39, 4, 529, 48, 25, 181, 8, 67, 35, 1732, 22, 49, 238, 60, 135, 1162, 14, 9, 290, 4, 58, 10, 10, 472, 45, 55, 878, 8, 169, 11, 374, 5687, 25, 203, 28, 8, 818, 12, 125, 4, 3077]),\n",
       "       list([1, 111, 748, 4368, 1133, 2, 2, 4, 87, 1551, 1262, 7, 31, 318, 9459, 7, 4, 498, 5076, 748, 63, 29, 5161, 220, 686, 10941, 5, 17, 12, 575, 220, 2507, 17, 6, 185, 132, 2, 16, 53, 928, 11, 2, 74, 4, 438, 21, 27, 10044, 589, 8, 22, 107, 2, 19550, 997, 1638, 8, 35, 2076, 9019, 11, 22, 231, 54, 29, 1706, 29, 100, 18995, 2425, 34, 12998, 8738, 2, 5, 19353, 98, 31, 2122, 33, 6, 58, 14, 3808, 1638, 8, 4, 365, 7, 2789, 3761, 356, 346, 4, 2, 1060, 63, 29, 93, 11, 5421, 11, 15236, 33, 6, 58, 54, 1270, 431, 748, 7, 32, 2580, 16, 11, 94, 19469, 10, 10, 4, 993, 2, 7, 4, 1766, 2634, 2164, 2, 8, 847, 8, 1450, 121, 31, 7, 27, 86, 2663, 10760, 16, 6, 465, 993, 2006, 2, 573, 17, 2, 42, 4, 17345, 37, 473, 6, 711, 6, 8869, 7, 328, 212, 70, 30, 258, 11, 220, 32, 7, 108, 21, 133, 12, 9, 55, 465, 849, 3711, 53, 33, 2071, 1969, 37, 70, 1144, 4, 5940, 1409, 74, 476, 37, 62, 91, 1329, 169, 4, 1330, 10104, 146, 655, 2212, 5, 258, 12, 184, 10104, 546, 5, 849, 10333, 7, 4, 22, 1436, 18, 631, 1386, 797, 7, 4, 8712, 71, 348, 425, 4320, 1061, 19, 10288, 5, 12141, 11, 661, 8, 339, 17863, 4, 2455, 11434, 7, 4, 1962, 10, 10, 263, 787, 9, 270, 11, 6, 9466, 4, 2, 2, 121, 4, 5437, 26, 4434, 19, 68, 1372, 5, 28, 446, 6, 318, 7149, 8, 67, 51, 36, 70, 81, 8, 4392, 2294, 36, 1197, 8, 2, 2, 18, 6, 711, 4, 9909, 26, 10296, 1125, 11, 14, 636, 720, 12, 426, 28, 77, 776, 8, 97, 38, 111, 7489, 6175, 168, 1239, 5189, 137, 2, 18, 27, 173, 9, 2399, 17, 6, 12397, 428, 14657, 232, 11, 4, 8014, 37, 272, 40, 2708, 247, 30, 656, 6, 13182, 54, 2, 3292, 98, 6, 2840, 40, 558, 37, 6093, 98, 4, 17345, 1197, 15, 14, 9, 57, 4893, 5, 4659, 6, 275, 711, 7937, 2, 3292, 98, 6, 2, 10, 10, 6639, 19, 14, 10241, 267, 162, 711, 37, 5900, 752, 98, 4, 17345, 2378, 90, 19, 6, 2, 7, 2, 1810, 2, 4, 4770, 3183, 930, 8, 508, 90, 4, 1317, 8, 4, 2, 17, 15454, 3965, 1853, 4, 1494, 8, 4468, 189, 4, 2, 6287, 5774, 4, 4770, 5, 95, 271, 23, 6, 7742, 6063, 2, 5437, 33, 1526, 6, 425, 3155, 2, 4535, 1636, 7, 4, 4669, 11966, 469, 4, 4552, 54, 4, 150, 5664, 17345, 280, 53, 2, 2, 18, 339, 29, 1978, 27, 7885, 5, 17303, 68, 1830, 19, 6571, 14605, 4, 1515, 7, 263, 65, 2132, 34, 6, 5680, 7489, 43, 159, 29, 9, 4706, 9, 387, 73, 195, 584, 10, 10, 1069, 4, 58, 810, 54, 14, 6078, 117, 22, 16, 93, 5, 1069, 4, 192, 15, 12, 16, 93, 34, 6, 1766, 2, 33, 4, 5673, 7, 15, 18760, 9252, 3286, 325, 12, 62, 30, 776, 8, 67, 14, 17, 6, 12214, 44, 148, 687, 2, 203, 42, 203, 24, 28, 69, 2, 6676, 11, 330, 54, 29, 93, 2, 21, 845, 14148, 27, 1099, 7, 819, 4, 22, 1407, 17, 6, 14967, 787, 7, 2460, 19569, 2, 100, 30, 4, 3737, 3617, 3169, 2321, 42, 1898, 11, 4, 3814, 42, 101, 704, 7, 101, 999, 15, 1625, 94, 2926, 180, 5, 9, 9101, 34, 15205, 45, 6, 1429, 22, 60, 6, 1220, 31, 11, 94, 6408, 96, 21, 94, 749, 9, 57, 975]),\n",
       "       ...,\n",
       "       list([1, 13, 1408, 15, 8, 135, 14, 9, 35, 32, 46, 394, 20, 62, 30, 5093, 21, 45, 184, 78, 4, 1492, 910, 769, 2290, 2515, 395, 4257, 5, 1454, 11, 119, 16946, 89, 1036, 4, 116, 218, 78, 21, 407, 100, 30, 128, 262, 15, 7, 185, 2280, 284, 1842, 2, 37, 315, 4, 226, 20, 272, 2942, 40, 29, 152, 60, 181, 8, 30, 50, 553, 362, 80, 119, 12, 21, 846, 5518]),\n",
       "       list([1, 11, 119, 241, 9, 4, 840, 20, 12, 468, 15, 94, 3684, 562, 791, 39, 4, 86, 107, 8, 97, 14, 31, 33, 4, 2960, 7, 743, 46, 1028, 9, 3531, 5, 4, 768, 47, 8, 79, 90, 145, 164, 162, 50, 6, 501, 119, 7, 9, 4, 78, 232, 15, 16, 224, 11, 4, 333, 20, 4, 985, 200, 5, 2, 5, 9, 1861, 8, 79, 357, 4, 20, 47, 220, 57, 206, 139, 11, 12, 5, 55, 117, 212, 13, 1276, 92, 124, 51, 45, 1188, 71, 536, 13, 520, 14, 20, 6, 2302, 7, 470]),\n",
       "       list([1, 6, 52, 7465, 430, 22, 9, 220, 2594, 8, 28, 2, 519, 3227, 6, 769, 15, 47, 6, 3482, 4067, 8, 114, 5, 33, 222, 31, 55, 184, 704, 5586, 18020, 19, 346, 3153, 5, 6, 364, 350, 4, 184, 5586, 9, 133, 1810, 11, 5417, 13226, 21, 4, 7298, 2, 570, 50, 2005, 2643, 9, 6, 1249, 17, 6, 2, 2, 21, 17, 6, 1211, 232, 1138, 2249, 29, 266, 56, 96, 346, 194, 308, 9, 194, 21, 29, 218, 1078, 19, 4, 78, 173, 7, 27, 2, 5698, 3406, 718, 2, 9, 6, 6907, 17, 210, 5, 3281, 5677, 47, 77, 395, 14, 172, 173, 18, 2740, 2931, 4517, 82, 127, 27, 173, 11, 6, 392, 217, 21, 50, 9, 57, 65, 12, 14274, 53, 40, 35, 390, 7, 11, 4, 3567, 7, 4, 314, 74, 6, 792, 22, 16261, 19, 714, 727, 5205, 382, 4, 91, 6533, 439, 19, 14, 20, 9, 1441, 5805, 1118, 4, 756, 25, 124, 4, 31, 12, 16, 93, 804, 34, 2005, 2643])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad sequences (samples x time)\n",
      "x_train2 shape: (25000, 170)\n",
      "x_test2 shape: (25000, 170)\n"
     ]
    }
   ],
   "source": [
    "print('Pad sequences (samples x time)')\n",
    "x_train2 = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test2 = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train2 shape:', x_train2.shape)\n",
    "print('x_test2 shape:', x_test2.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list([1, 591, 202, 14, 31, 6, 717, 10, 10, 18142, 10698, 5, 4, 360, 7, 4, 177, 5760, 394, 354, 4, 123, 9, 1035, 1035, 1035, 10, 10, 13, 92, 124, 89, 488, 7944, 100, 28, 1668, 14, 31, 23, 27, 7479, 29, 220, 468, 8, 124, 14, 286, 170, 8, 157, 46, 5, 27, 239, 16, 179, 15387, 38, 32, 25, 7944, 451, 202, 14, 6, 717]),\n",
       "       list([1, 14, 22, 3443, 6, 176, 7, 5063, 88, 12, 2679, 23, 1310, 5, 109, 943, 4, 114, 9, 55, 606, 5, 111, 7, 4, 139, 193, 273, 23, 4, 172, 270, 11, 7216, 10626, 4, 8463, 2801, 109, 1603, 21, 4, 22, 3861, 8, 6, 1193, 1330, 10, 10, 4, 105, 987, 35, 841, 16873, 19, 861, 1074, 5, 1987, 17975, 45, 55, 221, 15, 670, 5304, 526, 14, 1069, 4, 405, 5, 2438, 7, 27, 85, 108, 131, 4, 5045, 5304, 3884, 405, 9, 3523, 133, 5, 50, 13, 104, 51, 66, 166, 14, 22, 157, 9, 4, 530, 239, 34, 8463, 2801, 45, 407, 31, 7, 41, 3778, 105, 21, 59, 299, 12, 38, 950, 5, 4521, 15, 45, 629, 488, 2733, 127, 6, 52, 292, 17, 4, 6936, 185, 132, 1988, 5304, 1799, 488, 2693, 47, 6, 392, 173, 4, 2, 4378, 270, 2352, 4, 1500, 7, 4, 65, 55, 73, 11, 346, 14, 20, 9, 6, 976, 2078, 7, 5293, 861, 12746, 5, 4182, 30, 3127, 2, 56, 4, 841, 5, 990, 692, 8, 4, 1669, 398, 229, 10, 10, 13, 2822, 670, 5304, 14, 9, 31, 7, 27, 111, 108, 15, 2033, 19, 7836, 1429, 875, 551, 14, 22, 9, 1193, 21, 45, 4829, 5, 45, 252, 8, 12508, 6, 565, 921, 3639, 39, 4, 529, 48, 25, 181, 8, 67, 35, 1732, 22, 49, 238, 60, 135, 1162, 14, 9, 290, 4, 58, 10, 10, 472, 45, 55, 878, 8, 169, 11, 374, 5687, 25, 203, 28, 8, 818, 12, 125, 4, 3077]),\n",
       "       list([1, 111, 748, 4368, 1133, 2, 2, 4, 87, 1551, 1262, 7, 31, 318, 9459, 7, 4, 498, 5076, 748, 63, 29, 5161, 220, 686, 10941, 5, 17, 12, 575, 220, 2507, 17, 6, 185, 132, 2, 16, 53, 928, 11, 2, 74, 4, 438, 21, 27, 10044, 589, 8, 22, 107, 2, 19550, 997, 1638, 8, 35, 2076, 9019, 11, 22, 231, 54, 29, 1706, 29, 100, 18995, 2425, 34, 12998, 8738, 2, 5, 19353, 98, 31, 2122, 33, 6, 58, 14, 3808, 1638, 8, 4, 365, 7, 2789, 3761, 356, 346, 4, 2, 1060, 63, 29, 93, 11, 5421, 11, 15236, 33, 6, 58, 54, 1270, 431, 748, 7, 32, 2580, 16, 11, 94, 19469, 10, 10, 4, 993, 2, 7, 4, 1766, 2634, 2164, 2, 8, 847, 8, 1450, 121, 31, 7, 27, 86, 2663, 10760, 16, 6, 465, 993, 2006, 2, 573, 17, 2, 42, 4, 17345, 37, 473, 6, 711, 6, 8869, 7, 328, 212, 70, 30, 258, 11, 220, 32, 7, 108, 21, 133, 12, 9, 55, 465, 849, 3711, 53, 33, 2071, 1969, 37, 70, 1144, 4, 5940, 1409, 74, 476, 37, 62, 91, 1329, 169, 4, 1330, 10104, 146, 655, 2212, 5, 258, 12, 184, 10104, 546, 5, 849, 10333, 7, 4, 22, 1436, 18, 631, 1386, 797, 7, 4, 8712, 71, 348, 425, 4320, 1061, 19, 10288, 5, 12141, 11, 661, 8, 339, 17863, 4, 2455, 11434, 7, 4, 1962, 10, 10, 263, 787, 9, 270, 11, 6, 9466, 4, 2, 2, 121, 4, 5437, 26, 4434, 19, 68, 1372, 5, 28, 446, 6, 318, 7149, 8, 67, 51, 36, 70, 81, 8, 4392, 2294, 36, 1197, 8, 2, 2, 18, 6, 711, 4, 9909, 26, 10296, 1125, 11, 14, 636, 720, 12, 426, 28, 77, 776, 8, 97, 38, 111, 7489, 6175, 168, 1239, 5189, 137, 2, 18, 27, 173, 9, 2399, 17, 6, 12397, 428, 14657, 232, 11, 4, 8014, 37, 272, 40, 2708, 247, 30, 656, 6, 13182, 54, 2, 3292, 98, 6, 2840, 40, 558, 37, 6093, 98, 4, 17345, 1197, 15, 14, 9, 57, 4893, 5, 4659, 6, 275, 711, 7937, 2, 3292, 98, 6, 2, 10, 10, 6639, 19, 14, 10241, 267, 162, 711, 37, 5900, 752, 98, 4, 17345, 2378, 90, 19, 6, 2, 7, 2, 1810, 2, 4, 4770, 3183, 930, 8, 508, 90, 4, 1317, 8, 4, 2, 17, 15454, 3965, 1853, 4, 1494, 8, 4468, 189, 4, 2, 6287, 5774, 4, 4770, 5, 95, 271, 23, 6, 7742, 6063, 2, 5437, 33, 1526, 6, 425, 3155, 2, 4535, 1636, 7, 4, 4669, 11966, 469, 4, 4552, 54, 4, 150, 5664, 17345, 280, 53, 2, 2, 18, 339, 29, 1978, 27, 7885, 5, 17303, 68, 1830, 19, 6571, 14605, 4, 1515, 7, 263, 65, 2132, 34, 6, 5680, 7489, 43, 159, 29, 9, 4706, 9, 387, 73, 195, 584, 10, 10, 1069, 4, 58, 810, 54, 14, 6078, 117, 22, 16, 93, 5, 1069, 4, 192, 15, 12, 16, 93, 34, 6, 1766, 2, 33, 4, 5673, 7, 15, 18760, 9252, 3286, 325, 12, 62, 30, 776, 8, 67, 14, 17, 6, 12214, 44, 148, 687, 2, 203, 42, 203, 24, 28, 69, 2, 6676, 11, 330, 54, 29, 93, 2, 21, 845, 14148, 27, 1099, 7, 819, 4, 22, 1407, 17, 6, 14967, 787, 7, 2460, 19569, 2, 100, 30, 4, 3737, 3617, 3169, 2321, 42, 1898, 11, 4, 3814, 42, 101, 704, 7, 101, 999, 15, 1625, 94, 2926, 180, 5, 9, 9101, 34, 15205, 45, 6, 1429, 22, 60, 6, 1220, 31, 11, 94, 6408, 96, 21, 94, 749, 9, 57, 975]),\n",
       "       ...,\n",
       "       list([1, 13, 1408, 15, 8, 135, 14, 9, 35, 32, 46, 394, 20, 62, 30, 5093, 21, 45, 184, 78, 4, 1492, 910, 769, 2290, 2515, 395, 4257, 5, 1454, 11, 119, 16946, 89, 1036, 4, 116, 218, 78, 21, 407, 100, 30, 128, 262, 15, 7, 185, 2280, 284, 1842, 2, 37, 315, 4, 226, 20, 272, 2942, 40, 29, 152, 60, 181, 8, 30, 50, 553, 362, 80, 119, 12, 21, 846, 5518]),\n",
       "       list([1, 11, 119, 241, 9, 4, 840, 20, 12, 468, 15, 94, 3684, 562, 791, 39, 4, 86, 107, 8, 97, 14, 31, 33, 4, 2960, 7, 743, 46, 1028, 9, 3531, 5, 4, 768, 47, 8, 79, 90, 145, 164, 162, 50, 6, 501, 119, 7, 9, 4, 78, 232, 15, 16, 224, 11, 4, 333, 20, 4, 985, 200, 5, 2, 5, 9, 1861, 8, 79, 357, 4, 20, 47, 220, 57, 206, 139, 11, 12, 5, 55, 117, 212, 13, 1276, 92, 124, 51, 45, 1188, 71, 536, 13, 520, 14, 20, 6, 2302, 7, 470]),\n",
       "       list([1, 6, 52, 7465, 430, 22, 9, 220, 2594, 8, 28, 2, 519, 3227, 6, 769, 15, 47, 6, 3482, 4067, 8, 114, 5, 33, 222, 31, 55, 184, 704, 5586, 18020, 19, 346, 3153, 5, 6, 364, 350, 4, 184, 5586, 9, 133, 1810, 11, 5417, 13226, 21, 4, 7298, 2, 570, 50, 2005, 2643, 9, 6, 1249, 17, 6, 2, 2, 21, 17, 6, 1211, 232, 1138, 2249, 29, 266, 56, 96, 346, 194, 308, 9, 194, 21, 29, 218, 1078, 19, 4, 78, 173, 7, 27, 2, 5698, 3406, 718, 2, 9, 6, 6907, 17, 210, 5, 3281, 5677, 47, 77, 395, 14, 172, 173, 18, 2740, 2931, 4517, 82, 127, 27, 173, 11, 6, 392, 217, 21, 50, 9, 57, 65, 12, 14274, 53, 40, 35, 390, 7, 11, 4, 3567, 7, 4, 314, 74, 6, 792, 22, 16261, 19, 714, 727, 5205, 382, 4, 91, 6533, 439, 19, 14, 20, 9, 1441, 5805, 1118, 4, 756, 25, 124, 4, 31, 12, 16, 93, 804, 34, 2005, 2643])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128, name=\"input_layer\"))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid', name=\"output_layer\"))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_layer (Embedding)      (None, None, 128)         2560000   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "output_layer (Dense)         (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 2,691,713\n",
      "Trainable params: 2,691,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard = TensorBoard(log_dir=\"logs/{}\".format(time()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/15\n",
      "25000/25000 [==============================] - 81s 3ms/step - loss: 0.4714 - acc: 0.7764 - val_loss: 0.3697 - val_acc: 0.8427\n",
      "Epoch 2/15\n",
      "25000/25000 [==============================] - 82s 3ms/step - loss: 0.3110 - acc: 0.8741 - val_loss: 0.3415 - val_acc: 0.8539\n",
      "Epoch 3/15\n",
      "25000/25000 [==============================] - 83s 3ms/step - loss: 0.2186 - acc: 0.9166 - val_loss: 0.3533 - val_acc: 0.8593\n",
      "Epoch 4/15\n",
      "25000/25000 [==============================] - 83s 3ms/step - loss: 0.1726 - acc: 0.9356 - val_loss: 0.3852 - val_acc: 0.8603\n",
      "Epoch 5/15\n",
      "25000/25000 [==============================] - 84s 3ms/step - loss: 0.1288 - acc: 0.9533 - val_loss: 0.4197 - val_acc: 0.8486\n",
      "Epoch 6/15\n",
      "25000/25000 [==============================] - 83s 3ms/step - loss: 0.1130 - acc: 0.9591 - val_loss: 0.4983 - val_acc: 0.8505\n",
      "Epoch 7/15\n",
      "25000/25000 [==============================] - 80s 3ms/step - loss: 0.0696 - acc: 0.9764 - val_loss: 0.5528 - val_acc: 0.8469\n",
      "Epoch 8/15\n",
      "25000/25000 [==============================] - 79s 3ms/step - loss: 0.0465 - acc: 0.9851 - val_loss: 0.6164 - val_acc: 0.8474\n",
      "Epoch 9/15\n",
      "25000/25000 [==============================] - 80s 3ms/step - loss: 0.0400 - acc: 0.9870 - val_loss: 0.6222 - val_acc: 0.8484\n",
      "Epoch 10/15\n",
      "25000/25000 [==============================] - 80s 3ms/step - loss: 0.0297 - acc: 0.9903 - val_loss: 0.7057 - val_acc: 0.8481\n",
      "Epoch 11/15\n",
      "25000/25000 [==============================] - 80s 3ms/step - loss: 0.0214 - acc: 0.9931 - val_loss: 0.8064 - val_acc: 0.8431\n",
      "Epoch 12/15\n",
      "25000/25000 [==============================] - 82s 3ms/step - loss: 0.0256 - acc: 0.9919 - val_loss: 0.6854 - val_acc: 0.8295\n",
      "Epoch 13/15\n",
      "25000/25000 [==============================] - 82s 3ms/step - loss: 0.0243 - acc: 0.9922 - val_loss: 0.7806 - val_acc: 0.8450\n",
      "Epoch 14/15\n",
      "25000/25000 [==============================] - 83s 3ms/step - loss: 0.0098 - acc: 0.9976 - val_loss: 0.7492 - val_acc: 0.8409\n",
      "Epoch 15/15\n",
      "25000/25000 [==============================] - 84s 3ms/step - loss: 0.0092 - acc: 0.9972 - val_loss: 0.9127 - val_acc: 0.8408\n",
      "25000/25000 [==============================] - 12s 467us/step\n",
      "Test score: 0.9127236043155194\n",
      "Test accuracy: 0.84076\n"
     ]
    }
   ],
   "source": [
    "print('Train...')\n",
    "model.fit(x_train2, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=15,\n",
    "          validation_data=(x_test2, y_test),\n",
    "          verbose=1,\n",
    "          callbacks = [tensorboard])\n",
    "\n",
    "score, acc = model.evaluate(x_test2, y_test,\n",
    "          batch_size=batch_size)\n",
    "\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tensorboard --logdir=logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_layer_input\n",
      "input_layer/random_uniform/shape\n",
      "input_layer/random_uniform/min\n",
      "input_layer/random_uniform/max\n",
      "input_layer/random_uniform/RandomUniform\n",
      "input_layer/random_uniform/sub\n",
      "input_layer/random_uniform/mul\n",
      "input_layer/random_uniform\n",
      "input_layer/embeddings\n",
      "input_layer/embeddings/Assign\n",
      "input_layer/embeddings/read\n",
      "input_layer/Cast\n",
      "input_layer/embedding_lookup/axis\n",
      "input_layer/embedding_lookup\n",
      "output_layer/random_uniform/shape\n",
      "output_layer/random_uniform/min\n",
      "output_layer/random_uniform/max\n",
      "output_layer/random_uniform/RandomUniform\n",
      "output_layer/random_uniform/sub\n",
      "output_layer/random_uniform/mul\n",
      "output_layer/random_uniform\n",
      "output_layer/kernel\n",
      "output_layer/kernel/Assign\n",
      "output_layer/kernel/read\n",
      "output_layer/Const\n",
      "output_layer/bias\n",
      "output_layer/bias/Assign\n",
      "output_layer/bias/read\n",
      "output_layer/MatMul\n",
      "output_layer/BiasAdd\n",
      "output_layer/Sigmoid\n",
      "output_layer_target\n",
      "output_layer_sample_weights\n",
      "loss/output_layer_loss/Const\n",
      "loss/output_layer_loss/sub/x\n",
      "loss/output_layer_loss/sub\n",
      "loss/output_layer_loss/clip_by_value\n",
      "loss/output_layer_loss/sub_1/x\n",
      "loss/output_layer_loss/sub_1\n",
      "loss/output_layer_loss/truediv\n",
      "loss/output_layer_loss/Log\n",
      "loss/output_layer_loss/logistic_loss/zeros_like\n",
      "loss/output_layer_loss/logistic_loss/GreaterEqual\n",
      "loss/output_layer_loss/logistic_loss/Select\n",
      "loss/output_layer_loss/logistic_loss/Neg\n",
      "loss/output_layer_loss/logistic_loss/Select_1\n",
      "loss/output_layer_loss/logistic_loss/mul\n",
      "loss/output_layer_loss/logistic_loss/sub\n",
      "loss/output_layer_loss/logistic_loss/Exp\n",
      "loss/output_layer_loss/logistic_loss/Log1p\n",
      "loss/output_layer_loss/logistic_loss\n",
      "loss/output_layer_loss/Mean/reduction_indices\n",
      "loss/output_layer_loss/Mean\n",
      "loss/output_layer_loss/Mean_1/reduction_indices\n",
      "loss/output_layer_loss/Mean_1\n",
      "loss/output_layer_loss/mul\n",
      "loss/output_layer_loss/NotEqual/y\n",
      "loss/output_layer_loss/NotEqual\n",
      "loss/output_layer_loss/Cast\n",
      "loss/output_layer_loss/Const_1\n",
      "loss/output_layer_loss/Mean_2\n",
      "loss/output_layer_loss/truediv_1\n",
      "loss/output_layer_loss/Const_2\n",
      "loss/output_layer_loss/Mean_3\n",
      "training/Adam/gradients/loss/output_layer_loss/Mean_3_grad/Reshape/shape\n",
      "training/Adam/gradients/loss/output_layer_loss/Mean_3_grad/Reshape\n",
      "training/Adam/gradients/loss/output_layer_loss/Mean_3_grad/Shape\n",
      "training/Adam/gradients/loss/output_layer_loss/Mean_3_grad/Tile\n",
      "training/Adam/gradients/loss/output_layer_loss/Mean_3_grad/Shape_1\n",
      "training/Adam/gradients/loss/output_layer_loss/Mean_3_grad/Shape_2\n",
      "training/Adam/gradients/loss/output_layer_loss/Mean_3_grad/Const\n",
      "training/Adam/gradients/loss/output_layer_loss/Mean_3_grad/Prod\n",
      "training/Adam/gradients/loss/output_layer_loss/Mean_3_grad/Const_1\n",
      "training/Adam/gradients/loss/output_layer_loss/Mean_3_grad/Prod_1\n",
      "training/Adam/gradients/loss/output_layer_loss/Mean_3_grad/Maximum/y\n",
      "training/Adam/gradients/loss/output_layer_loss/Mean_3_grad/Maximum\n",
      "training/Adam/gradients/loss/output_layer_loss/Mean_3_grad/floordiv\n",
      "training/Adam/gradients/loss/output_layer_loss/Mean_3_grad/Cast\n",
      "training/Adam/gradients/loss/output_layer_loss/Mean_3_grad/truediv\n",
      "training/Adam/gradients/loss/output_layer_loss/truediv_1_grad/Shape\n",
      "training/Adam/gradients/loss/output_layer_loss/truediv_1_grad/Shape_1\n",
      "training/Adam/gradients/loss/output_layer_loss/truediv_1_grad/BroadcastGradientArgs\n",
      "training/Adam/gradients/loss/output_layer_loss/truediv_1_grad/RealDiv\n",
      "training/Adam/gradients/loss/output_layer_loss/truediv_1_grad/Sum\n",
      "training/Adam/gradients/loss/output_layer_loss/truediv_1_grad/Reshape\n",
      "training/Adam/gradients/loss/output_layer_loss/truediv_1_grad/Neg\n",
      "training/Adam/gradients/loss/output_layer_loss/truediv_1_grad/RealDiv_1\n",
      "training/Adam/gradients/loss/output_layer_loss/truediv_1_grad/RealDiv_2\n",
      "training/Adam/gradients/loss/output_layer_loss/truediv_1_grad/mul\n",
      "training/Adam/gradients/loss/output_layer_loss/truediv_1_grad/Sum_1\n",
      "training/Adam/gradients/loss/output_layer_loss/truediv_1_grad/Reshape_1\n",
      "training/Adam/gradients/loss/output_layer_loss/mul_grad/Shape\n",
      "training/Adam/gradients/loss/output_layer_loss/mul_grad/Shape_1\n",
      "training/Adam/gradients/loss/output_layer_loss/mul_grad/BroadcastGradientArgs\n",
      "training/Adam/gradients/loss/output_layer_loss/mul_grad/Mul\n",
      "training/Adam/gradients/loss/output_layer_loss/mul_grad/Sum\n",
      "training/Adam/gradients/loss/output_layer_loss/mul_grad/Reshape\n",
      "training/Adam/gradients/loss/output_layer_loss/mul_grad/Mul_1\n",
      "training/Adam/gradients/loss/output_layer_loss/mul_grad/Sum_1\n",
      "training/Adam/gradients/loss/output_layer_loss/mul_grad/Reshape_1\n",
      "training/Adam/gradients/loss/output_layer_loss/Mean_1_grad/Shape\n",
      "training/Adam/gradients/loss/output_layer_loss/Mean_1_grad/Size\n",
      "training/Adam/gradients/loss/output_layer_loss/Mean_1_grad/add\n",
      "training/Adam/gradients/loss/output_layer_loss/Mean_1_grad/mod\n",
      "training/Adam/gradients/loss/output_layer_loss/Mean_1_grad/Shape_1\n",
      "training/Adam/gradients/loss/output_layer_loss/Mean_1_grad/range/start\n",
      "training/Adam/gradients/loss/output_layer_loss/Mean_1_grad/range/delta\n",
      "training/Adam/gradients/loss/output_layer_loss/Mean_1_grad/range\n",
      "training/Adam/gradients/loss/output_layer_loss/Mean_1_grad/Fill/value\n",
      "training/Adam/gradients/loss/output_layer_loss/Mean_1_grad/Fill\n",
      "training/Adam/gradients/loss/output_layer_loss/Mean_1_grad/DynamicStitch\n",
      "training/Adam/gradients/loss/output_layer_loss/Mean_1_grad/Maximum/y\n",
      "training/Adam/gradients/loss/output_layer_loss/Mean_1_grad/Maximum\n",
      "training/Adam/gradients/loss/output_layer_loss/Mean_1_grad/floordiv\n",
      "training/Adam/gradients/loss/output_layer_loss/Mean_1_grad/Reshape\n",
      "training/Adam/gradients/loss/output_layer_loss/Mean_1_grad/Tile\n",
      "training/Adam/gradients/loss/output_layer_loss/Mean_1_grad/Shape_2\n",
      "training/Adam/gradients/loss/output_layer_loss/Mean_1_grad/Shape_3\n",
      "training/Adam/gradients/loss/output_layer_loss/Mean_1_grad/Const\n",
      "training/Adam/gradients/loss/output_layer_loss/Mean_1_grad/Prod\n",
      "training/Adam/gradients/loss/output_layer_loss/Mean_1_grad/Const_1\n",
      "training/Adam/gradients/loss/output_layer_loss/Mean_1_grad/Prod_1\n",
      "training/Adam/gradients/loss/output_layer_loss/Mean_1_grad/Maximum_1/y\n",
      "training/Adam/gradients/loss/output_layer_loss/Mean_1_grad/Maximum_1\n",
      "training/Adam/gradients/loss/output_layer_loss/Mean_1_grad/floordiv_1\n",
      "training/Adam/gradients/loss/output_layer_loss/Mean_1_grad/Cast\n",
      "training/Adam/gradients/loss/output_layer_loss/Mean_1_grad/truediv\n",
      "training/Adam/gradients/loss/output_layer_loss/Mean_grad/Shape\n",
      "training/Adam/gradients/loss/output_layer_loss/Mean_grad/Size\n",
      "training/Adam/gradients/loss/output_layer_loss/Mean_grad/add\n",
      "training/Adam/gradients/loss/output_layer_loss/Mean_grad/mod\n",
      "training/Adam/gradients/loss/output_layer_loss/Mean_grad/Shape_1\n",
      "training/Adam/gradients/loss/output_layer_loss/Mean_grad/range/start\n",
      "training/Adam/gradients/loss/output_layer_loss/Mean_grad/range/delta\n",
      "training/Adam/gradients/loss/output_layer_loss/Mean_grad/range\n",
      "training/Adam/gradients/loss/output_layer_loss/Mean_grad/Fill/value\n",
      "training/Adam/gradients/loss/output_layer_loss/Mean_grad/Fill\n",
      "training/Adam/gradients/loss/output_layer_loss/Mean_grad/DynamicStitch\n",
      "training/Adam/gradients/loss/output_layer_loss/Mean_grad/Maximum/y\n",
      "training/Adam/gradients/loss/output_layer_loss/Mean_grad/Maximum\n",
      "training/Adam/gradients/loss/output_layer_loss/Mean_grad/floordiv\n",
      "training/Adam/gradients/loss/output_layer_loss/Mean_grad/Reshape\n",
      "training/Adam/gradients/loss/output_layer_loss/Mean_grad/Tile\n",
      "training/Adam/gradients/loss/output_layer_loss/Mean_grad/Shape_2\n",
      "training/Adam/gradients/loss/output_layer_loss/Mean_grad/Shape_3\n",
      "training/Adam/gradients/loss/output_layer_loss/Mean_grad/Const\n",
      "training/Adam/gradients/loss/output_layer_loss/Mean_grad/Prod\n",
      "training/Adam/gradients/loss/output_layer_loss/Mean_grad/Const_1\n",
      "training/Adam/gradients/loss/output_layer_loss/Mean_grad/Prod_1\n",
      "training/Adam/gradients/loss/output_layer_loss/Mean_grad/Maximum_1/y\n",
      "training/Adam/gradients/loss/output_layer_loss/Mean_grad/Maximum_1\n",
      "training/Adam/gradients/loss/output_layer_loss/Mean_grad/floordiv_1\n",
      "training/Adam/gradients/loss/output_layer_loss/Mean_grad/Cast\n",
      "training/Adam/gradients/loss/output_layer_loss/Mean_grad/truediv\n",
      "training/Adam/gradients/loss/output_layer_loss/logistic_loss_grad/Shape\n",
      "training/Adam/gradients/loss/output_layer_loss/logistic_loss_grad/Shape_1\n",
      "training/Adam/gradients/loss/output_layer_loss/logistic_loss_grad/BroadcastGradientArgs\n",
      "training/Adam/gradients/loss/output_layer_loss/logistic_loss_grad/Sum\n",
      "training/Adam/gradients/loss/output_layer_loss/logistic_loss_grad/Reshape\n",
      "training/Adam/gradients/loss/output_layer_loss/logistic_loss_grad/Sum_1\n",
      "training/Adam/gradients/loss/output_layer_loss/logistic_loss_grad/Reshape_1\n",
      "training/Adam/gradients/loss/output_layer_loss/logistic_loss/sub_grad/Shape\n",
      "training/Adam/gradients/loss/output_layer_loss/logistic_loss/sub_grad/Shape_1\n",
      "training/Adam/gradients/loss/output_layer_loss/logistic_loss/sub_grad/BroadcastGradientArgs\n",
      "training/Adam/gradients/loss/output_layer_loss/logistic_loss/sub_grad/Sum\n",
      "training/Adam/gradients/loss/output_layer_loss/logistic_loss/sub_grad/Reshape\n",
      "training/Adam/gradients/loss/output_layer_loss/logistic_loss/sub_grad/Sum_1\n",
      "training/Adam/gradients/loss/output_layer_loss/logistic_loss/sub_grad/Neg\n",
      "training/Adam/gradients/loss/output_layer_loss/logistic_loss/sub_grad/Reshape_1\n",
      "training/Adam/gradients/loss/output_layer_loss/logistic_loss/Log1p_grad/add/x\n",
      "training/Adam/gradients/loss/output_layer_loss/logistic_loss/Log1p_grad/add\n",
      "training/Adam/gradients/loss/output_layer_loss/logistic_loss/Log1p_grad/Reciprocal\n",
      "training/Adam/gradients/loss/output_layer_loss/logistic_loss/Log1p_grad/mul\n",
      "training/Adam/gradients/loss/output_layer_loss/logistic_loss/Select_grad/zeros_like\n",
      "training/Adam/gradients/loss/output_layer_loss/logistic_loss/Select_grad/Select\n",
      "training/Adam/gradients/loss/output_layer_loss/logistic_loss/Select_grad/Select_1\n",
      "training/Adam/gradients/loss/output_layer_loss/logistic_loss/mul_grad/Shape\n",
      "training/Adam/gradients/loss/output_layer_loss/logistic_loss/mul_grad/Shape_1\n",
      "training/Adam/gradients/loss/output_layer_loss/logistic_loss/mul_grad/BroadcastGradientArgs\n",
      "training/Adam/gradients/loss/output_layer_loss/logistic_loss/mul_grad/Mul\n",
      "training/Adam/gradients/loss/output_layer_loss/logistic_loss/mul_grad/Sum\n",
      "training/Adam/gradients/loss/output_layer_loss/logistic_loss/mul_grad/Reshape\n",
      "training/Adam/gradients/loss/output_layer_loss/logistic_loss/mul_grad/Mul_1\n",
      "training/Adam/gradients/loss/output_layer_loss/logistic_loss/mul_grad/Sum_1\n",
      "training/Adam/gradients/loss/output_layer_loss/logistic_loss/mul_grad/Reshape_1\n",
      "training/Adam/gradients/loss/output_layer_loss/logistic_loss/Exp_grad/mul\n",
      "training/Adam/gradients/loss/output_layer_loss/logistic_loss/Select_1_grad/zeros_like\n",
      "training/Adam/gradients/loss/output_layer_loss/logistic_loss/Select_1_grad/Select\n",
      "training/Adam/gradients/loss/output_layer_loss/logistic_loss/Select_1_grad/Select_1\n",
      "training/Adam/gradients/loss/output_layer_loss/logistic_loss/Neg_grad/Neg\n",
      "training/Adam/gradients/loss/output_layer_loss/Log_grad/Reciprocal\n",
      "training/Adam/gradients/loss/output_layer_loss/Log_grad/mul\n",
      "training/Adam/gradients/loss/output_layer_loss/truediv_grad/Shape\n",
      "training/Adam/gradients/loss/output_layer_loss/truediv_grad/Shape_1\n",
      "training/Adam/gradients/loss/output_layer_loss/truediv_grad/BroadcastGradientArgs\n",
      "training/Adam/gradients/loss/output_layer_loss/truediv_grad/RealDiv\n",
      "training/Adam/gradients/loss/output_layer_loss/truediv_grad/Sum\n",
      "training/Adam/gradients/loss/output_layer_loss/truediv_grad/Reshape\n",
      "training/Adam/gradients/loss/output_layer_loss/truediv_grad/Neg\n",
      "training/Adam/gradients/loss/output_layer_loss/truediv_grad/RealDiv_1\n",
      "training/Adam/gradients/loss/output_layer_loss/truediv_grad/RealDiv_2\n",
      "training/Adam/gradients/loss/output_layer_loss/truediv_grad/mul\n",
      "training/Adam/gradients/loss/output_layer_loss/truediv_grad/Sum_1\n",
      "training/Adam/gradients/loss/output_layer_loss/truediv_grad/Reshape_1\n",
      "training/Adam/gradients/loss/output_layer_loss/sub_1_grad/Shape\n",
      "training/Adam/gradients/loss/output_layer_loss/sub_1_grad/Shape_1\n",
      "training/Adam/gradients/loss/output_layer_loss/sub_1_grad/BroadcastGradientArgs\n",
      "training/Adam/gradients/loss/output_layer_loss/sub_1_grad/Sum\n",
      "training/Adam/gradients/loss/output_layer_loss/sub_1_grad/Reshape\n",
      "training/Adam/gradients/loss/output_layer_loss/sub_1_grad/Sum_1\n",
      "training/Adam/gradients/loss/output_layer_loss/sub_1_grad/Neg\n",
      "training/Adam/gradients/loss/output_layer_loss/sub_1_grad/Reshape_1\n",
      "training/Adam/gradients/loss/output_layer_loss/clip_by_value_grad/Shape\n",
      "training/Adam/gradients/loss/output_layer_loss/clip_by_value_grad/Shape_1\n",
      "training/Adam/gradients/loss/output_layer_loss/clip_by_value_grad/Shape_2\n",
      "training/Adam/gradients/loss/output_layer_loss/clip_by_value_grad/Shape_3\n",
      "training/Adam/gradients/loss/output_layer_loss/clip_by_value_grad/zeros/Const\n",
      "training/Adam/gradients/loss/output_layer_loss/clip_by_value_grad/zeros\n",
      "training/Adam/gradients/loss/output_layer_loss/clip_by_value_grad/Less\n",
      "training/Adam/gradients/loss/output_layer_loss/clip_by_value_grad/Greater\n",
      "training/Adam/gradients/loss/output_layer_loss/clip_by_value_grad/BroadcastGradientArgs\n",
      "training/Adam/gradients/loss/output_layer_loss/clip_by_value_grad/BroadcastGradientArgs_1\n",
      "training/Adam/gradients/loss/output_layer_loss/clip_by_value_grad/LogicalOr\n",
      "training/Adam/gradients/loss/output_layer_loss/clip_by_value_grad/Select\n",
      "training/Adam/gradients/loss/output_layer_loss/clip_by_value_grad/Select_1\n",
      "training/Adam/gradients/loss/output_layer_loss/clip_by_value_grad/Select_2\n",
      "training/Adam/gradients/loss/output_layer_loss/clip_by_value_grad/Sum\n",
      "training/Adam/gradients/loss/output_layer_loss/clip_by_value_grad/Reshape\n",
      "training/Adam/gradients/loss/output_layer_loss/clip_by_value_grad/Sum_1\n",
      "training/Adam/gradients/loss/output_layer_loss/clip_by_value_grad/Reshape_1\n",
      "training/Adam/gradients/loss/output_layer_loss/clip_by_value_grad/Sum_2\n",
      "training/Adam/gradients/loss/output_layer_loss/clip_by_value_grad/Reshape_2\n",
      "training/Adam/gradients/output_layer/Sigmoid_grad/SigmoidGrad\n",
      "training/Adam/gradients/output_layer/BiasAdd_grad/BiasAddGrad\n",
      "training/Adam/gradients/output_layer/MatMul_grad/MatMul\n",
      "training/Adam/gradients/output_layer/MatMul_grad/MatMul_1\n",
      "training/Adam/gradients/input_layer/embedding_lookup_grad/Shape\n",
      "training/Adam/gradients/input_layer/embedding_lookup_grad/ToInt32\n",
      "training/Adam/gradients/input_layer/embedding_lookup_grad/Size\n",
      "training/Adam/gradients/input_layer/embedding_lookup_grad/ExpandDims/dim\n",
      "training/Adam/gradients/input_layer/embedding_lookup_grad/ExpandDims\n",
      "training/Adam/gradients/input_layer/embedding_lookup_grad/strided_slice/stack\n",
      "training/Adam/gradients/input_layer/embedding_lookup_grad/strided_slice/stack_1\n",
      "training/Adam/gradients/input_layer/embedding_lookup_grad/strided_slice/stack_2\n",
      "training/Adam/gradients/input_layer/embedding_lookup_grad/strided_slice\n",
      "training/Adam/gradients/input_layer/embedding_lookup_grad/concat/axis\n",
      "training/Adam/gradients/input_layer/embedding_lookup_grad/concat\n",
      "training/Adam/gradients/input_layer/embedding_lookup_grad/Reshape\n",
      "training/Adam/gradients/input_layer/embedding_lookup_grad/Reshape_1\n"
     ]
    }
   ],
   "source": [
    "# layer names\n",
    "import tensorflow as tf\n",
    "import keras as K\n",
    "\n",
    "sess = K.backend.get_session()\n",
    "for n in sess.graph.as_graph_def().node:\n",
    "    if 'input_layer' in n.name:\n",
    "          print(n.name)\n",
    "    if 'output_layer' in n.name:\n",
    "          print(n.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:No assets to save.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: b'imdb-lstm-model7/saved_model.pb'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "b'imdb-lstm-model7/saved_model.pb'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save the graph model\n",
    "builder = tf.saved_model.builder.SavedModelBuilder(\"imdb-lstm-model7\")\n",
    "builder.add_meta_graph_and_variables(sess, [\"server\"])\n",
    "builder.save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the graph model in hdf5\n",
    "model.save('imdb-lstm-model7.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original word index\n",
    "word_index = imdb.get_word_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1408"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# word index example\n",
    "word_index[\"woods\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from word index back to words\n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "def decode_review(text):\n",
    "    return ' '.join([reverse_word_index.get(i, '?') for i in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"the wonder own as by is sequence i i jars roses to of hollywood br of down shouting getting boring of ever it sadly sadly sadly i i was then does don't close faint after one carry as by are be favourites all family turn in does as three part in another some to be probably with world uncaring her an have faint beginning own as is sequence\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example back to words\n",
    "decode_review(x_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data = [\n",
    "    \"this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert  is an amazing actor and now the same being director  father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for  and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also  to the two little boy's that played the  of norman and paul they were just brilliant children are often left out of the  list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\",\n",
    "    \"To put this in context, I am 34 years old and I have to say that this is the best film I have seen without doubt and I don't expect it will be beaten as far as I am concerned. Obviously times move on, and I acknowledge that due to its violence and one particularly uncomfortable scene this film is not for everyone, but I still remember watching it for the first time, and it blew me away. Anyone who watches it now has to remember that it actually changed the history of cinema. In context- it followed a decade or more of action films that always ended with a chase sequence where the hero saved the day - you could have written those films yourself. Pulp had you gripped and credited the audience with intelligence. There is not a line of wasted dialogue and the movie incorporates a number of complexities that are not immediately obvious. It also resurrected the career of Grease icon John Travolta and highlighted the acting talent of Samuel L Jackson. There are many films now that are edited out of sequence and have multiple plots etc but this is the one they all want to be, or all want to beat, but never will.\",\n",
    "    \"Viewers are taken on a ride through three different stories that entertwine together around the world of Marcellus Wallace. Quentin Tarantino proves that he is the master of witty dialogue and a fast plot that doesn't allow the viewer a moment of boredom or rest. From the story of two hit-man on a job, to a fixed boxing match to a date between a hit-man and the wife of a mob boss. There was definitely a lot of care into the writing of the script, as everything no matter the order it is in, fits with the story. Many mysteries have been left such as what is inside of the briefcase and why Marcellus Wallace has a band-aid on the back of his neck, which may be connected. The movie redefined the action genre and reinvigorated the careers of both John Travolta and Bruce Willis. This movie is required viewing for any fan of film.\",\n",
    "    \"Was it the money? Did he owe someone a favour? Cage why are you in this terrible movie ? Left Behind is even worse than the Kirk Cameron version ,which also sucked. The book I am sure is better, since many have read it. This movie is a career killer ...it is sad to see Cage left behind by Hollywood. The acting is bad , Cage tries hard to make work out of a script that was seemingly penned by 8 year old's . I mean the dialogue is terrible. Why waste your money making a film that is just bad. Cage needs to really rethink his career . Honestly I felt ashamed for Nick . Wild at Heart , Leaving Las Vegas, and Oscar winner... My God man have you no self respect? Nick you are better than this.\",\n",
    "    \"This film is so banal it takes the banality out of The Banal. The premise is absurd. The unravelling of the plot is absurd. The performances are, at best, distracted as is the direction. Even the actors seem unconvinced. I'm still kicking myself for watching this film. Sin and purity are depicted with juvenile simplicity. Religious bias is arrogantly displayed (but I'll leave that to the unfortunate viewer to spot). Complex ideas like forgiveness, atonement and judgement are reduced to idiocy at an atomic level. I apologise if this sounds too much like a rant but for someone who watches well over 15 films a week, I have never subjected myself to anything this offensive (to the senses) in over 25 years.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tokenizer to preprocess our text descriptions\n",
    "tokenizer = K.preprocessing.text.Tokenizer(num_words=max_features, char_level=False)\n",
    "# tokenize.fit_on_texts(sample_data) # only fit on train\n",
    "tokenizer.word_index = word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_bow = tokenizer.texts_to_matrix(sample_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 1., ..., 0., 0., 0.],\n",
       "       [0., 1., 1., ..., 0., 0., 0.],\n",
       "       [0., 1., 1., ..., 0., 0., 0.],\n",
       "       [0., 1., 1., ..., 0., 0., 0.],\n",
       "       [0., 1., 1., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 20000)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_bow.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[11, 19, 13, 40, 527, 970, 1619, 1382, 62, 455, 4465, 63, 3938, 1, 170, 33, 253, 2, 22, 97, 40, 835, 109, 47, 667, 6, 32, 477, 281, 2, 147, 1, 169, 109, 164, 333, 382, 36, 1, 169, 4533, 1108, 14, 543, 35, 10, 444, 1, 189, 47, 13, 3, 144, 2022, 16, 11, 19, 1, 1917, 4610, 466, 1, 19, 68, 84, 9, 13, 40, 527, 35, 73, 12, 10, 1244, 1, 19, 14, 512, 14, 9, 13, 623, 15, 2, 59, 383, 9, 5, 313, 5, 103, 2, 1, 2220, 5241, 13, 477, 63, 3782, 30, 1, 127, 9, 13, 35, 616, 2, 22, 121, 48, 33, 132, 45, 22, 1412, 30, 3, 19, 9, 212, 25, 74, 49, 2, 11, 404, 13, 79, 5, 1, 104, 114, 5949, 12, 253, 1, 4, 3763, 2, 720, 33, 68, 40, 527, 473, 23, 397, 314, 43, 4, 1, 1026, 10, 101, 85, 1, 378, 12, 294, 95, 29, 2068, 53, 23, 138, 3, 191, 7483, 15, 1, 223, 19, 18, 131, 473, 23, 477, 2, 141, 27, 5532, 15, 48, 33, 25, 221, 89, 22, 101, 1, 223, 62, 13, 35, 1331, 85, 9, 13, 280, 2, 13, 4469, 110, 100, 29, 12, 13, 5342, 16, 175, 29], [5, 273, 11, 8, 2005, 10, 241, 12542, 150, 151, 2, 10, 25, 5, 132, 12, 11, 6, 1, 115, 19, 10, 25, 107, 206, 821, 2, 10, 89, 532, 9, 77, 27, 3604, 14, 227, 14, 10, 241, 1944, 537, 208, 844, 20, 2, 10, 7536, 12, 685, 5, 91, 564, 2, 28, 569, 3145, 133, 11, 19, 6, 21, 15, 313, 18, 10, 128, 374, 146, 9, 15, 1, 83, 55, 2, 9, 4277, 69, 242, 256, 34, 3628, 9, 147, 44, 5, 374, 12, 9, 162, 1191, 1, 476, 4, 435, 8, 2005, 9, 1474, 3, 2065, 39, 50, 4, 203, 105, 12, 207, 1051, 16, 3, 1310, 717, 118, 1, 629, 1891, 1, 248, 22, 97, 25, 395, 145, 105, 621, 3992, 66, 22, 15474, 2, 5328, 1, 308, 16, 1660, 47, 6, 21, 3, 344, 4, 1050, 411, 2, 1, 17, 13699, 3, 609, 4, 10378, 12, 23, 21, 1238, 575, 9, 79, 10253, 1, 608, 4, 8078, 4901, 305, 8585, 2, 11017, 1, 113, 673, 4, 7682, 2011, 1878, 47, 23, 108, 105, 147, 12, 23, 1990, 43, 4, 717, 2, 25, 2581, 1844, 522, 18, 11, 6, 1, 28, 33, 29, 178, 5, 27, 39, 29, 178, 5, 1556, 18, 112, 77], [794, 23, 620, 20, 3, 1362, 140, 286, 272, 534, 12, 292, 184, 1, 179, 4, 4137, 6629, 5733, 1531, 12, 26, 6, 1, 1297, 4, 1917, 411, 2, 3, 699, 111, 12, 149, 1738, 1, 526, 3, 558, 4, 3259, 39, 357, 36, 1, 62, 4, 104, 566, 129, 20, 3, 289, 5, 3, 8530, 3557, 1011, 5, 3, 1301, 197, 3, 566, 129, 2, 1, 319, 4, 3, 3021, 1407, 47, 13, 404, 3, 173, 4, 456, 80, 1, 484, 4, 1, 226, 14, 282, 54, 548, 1, 658, 9, 6, 8, 2349, 16, 1, 62, 108, 4160, 25, 74, 314, 138, 14, 48, 6, 1001, 4, 1, 18445, 2, 135, 4137, 44, 3, 1140, 4085, 20, 1, 142, 4, 24, 3304, 60, 200, 27, 3165, 1, 17, 1, 203, 509, 2, 1, 4073, 4, 196, 305, 8585, 2, 1475, 4953, 11, 17, 6, 2591, 826, 15, 98, 334, 4, 19], [13, 9, 1, 275, 119, 26, 8811, 291, 3, 5257, 1932, 135, 23, 22, 8, 11, 391, 17, 314, 493, 6, 57, 430, 71, 1, 4090, 4056, 307, 60, 79, 2064, 1, 271, 10, 241, 249, 6, 125, 234, 108, 25, 329, 9, 11, 17, 6, 3, 608, 452, 9, 6, 616, 5, 64, 1932, 314, 493, 31, 360, 1, 113, 6, 75, 1932, 494, 251, 5, 94, 154, 43, 4, 3, 226, 12, 13, 1570, 7713, 31, 706, 288, 17290, 10, 381, 1, 411, 6, 391, 135, 434, 126, 275, 228, 3, 19, 12, 6, 40, 75, 1932, 735, 5, 63, 15558, 24, 608, 1249, 10, 418, 3010, 15, 1848, 1355, 30, 480, 1197, 6254, 4535, 2, 732, 2283, 58, 555, 129, 25, 22, 54, 529, 1158, 1848, 22, 23, 125, 71, 11], [11, 19, 6, 35, 5607, 9, 301, 1, 12549, 43, 4, 1, 5607, 1, 860, 6, 1752, 1, 4, 1, 111, 6, 1752, 1, 351, 23, 30, 115, 7133, 14, 6, 1, 455, 57, 1, 153, 303, 143, 128, 4559, 543, 15, 146, 11, 19, 3071, 2, 14443, 23, 2396, 16, 3952, 4675, 1733, 7489, 6, 4339, 18, 634, 560, 12, 5, 1, 2408, 526, 5, 1463, 1312, 1005, 37, 7723, 2, 8404, 23, 3744, 5, 8798, 30, 32, 8882, 648, 10, 17168, 45, 11, 931, 96, 73, 37, 3, 8483, 18, 15, 291, 34, 3628, 70, 117, 1116, 105, 3, 1266, 10, 25, 112, 5155, 543, 5, 230, 11, 2400, 5, 1, 5187, 8, 117, 2473, 150]]\n",
      "[[ 1108    14   543    35    10   444     1   189    47    13     3   144\n",
      "   2022    16    11    19     1  1917  4610   466     1    19    68    84\n",
      "      9    13    40   527    35    73    12    10  1244     1    19    14\n",
      "    512    14     9    13   623    15     2    59   383     9     5   313\n",
      "      5   103     2     1  2220  5241    13   477    63  3782    30     1\n",
      "    127     9    13    35   616     2    22   121    48    33   132    45\n",
      "     22  1412    30     3    19     9   212    25    74    49     2    11\n",
      "    404    13    79     5     1   104   114  5949    12   253     1     4\n",
      "   3763     2   720    33    68    40   527   473    23   397   314    43\n",
      "      4     1  1026    10   101    85     1   378    12   294    95    29\n",
      "   2068    53    23   138     3   191  7483    15     1   223    19    18\n",
      "    131   473    23   477     2   141    27  5532    15    48    33    25\n",
      "    221    89    22   101     1   223    62    13    35  1331    85     9\n",
      "     13   280     2    13  4469   110   100    29    12    13  5342    16\n",
      "    175    29]\n",
      " [ 1944   537   208   844    20     2    10  7536    12   685     5    91\n",
      "    564     2    28   569  3145   133    11    19     6    21    15   313\n",
      "     18    10   128   374   146     9    15     1    83    55     2     9\n",
      "   4277    69   242   256    34  3628     9   147    44     5   374    12\n",
      "      9   162  1191     1   476     4   435     8  2005     9  1474     3\n",
      "   2065    39    50     4   203   105    12   207  1051    16     3  1310\n",
      "    717   118     1   629  1891     1   248    22    97    25   395   145\n",
      "    105   621  3992    66    22 15474     2  5328     1   308    16  1660\n",
      "     47     6    21     3   344     4  1050   411     2     1    17 13699\n",
      "      3   609     4 10378    12    23    21  1238   575     9    79 10253\n",
      "      1   608     4  8078  4901   305  8585     2 11017     1   113   673\n",
      "      4  7682  2011  1878    47    23   108   105   147    12    23  1990\n",
      "     43     4   717     2    25  2581  1844   522    18    11     6     1\n",
      "     28    33    29   178     5    27    39    29   178     5  1556    18\n",
      "    112    77]\n",
      " [  794    23   620    20     3  1362   140   286   272   534    12   292\n",
      "    184     1   179     4  4137  6629  5733  1531    12    26     6     1\n",
      "   1297     4  1917   411     2     3   699   111    12   149  1738     1\n",
      "    526     3   558     4  3259    39   357    36     1    62     4   104\n",
      "    566   129    20     3   289     5     3  8530  3557  1011     5     3\n",
      "   1301   197     3   566   129     2     1   319     4     3  3021  1407\n",
      "     47    13   404     3   173     4   456    80     1   484     4     1\n",
      "    226    14   282    54   548     1   658     9     6     8  2349    16\n",
      "      1    62   108  4160    25    74   314   138    14    48     6  1001\n",
      "      4     1 18445     2   135  4137    44     3  1140  4085    20     1\n",
      "    142     4    24  3304    60   200    27  3165     1    17     1   203\n",
      "    509     2     1  4073     4   196   305  8585     2  1475  4953    11\n",
      "     17     6  2591   826    15    98   334     4    19     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0]\n",
      " [   13     9     1   275   119    26  8811   291     3  5257  1932   135\n",
      "     23    22     8    11   391    17   314   493     6    57   430    71\n",
      "      1  4090  4056   307    60    79  2064     1   271    10   241   249\n",
      "      6   125   234   108    25   329     9    11    17     6     3   608\n",
      "    452     9     6   616     5    64  1932   314   493    31   360     1\n",
      "    113     6    75  1932   494   251     5    94   154    43     4     3\n",
      "    226    12    13  1570  7713    31   706   288 17290    10   381     1\n",
      "    411     6   391   135   434   126   275   228     3    19    12     6\n",
      "     40    75  1932   735     5    63 15558    24   608  1249    10   418\n",
      "   3010    15  1848  1355    30   480  1197  6254  4535     2   732  2283\n",
      "     58   555   129    25    22    54   529  1158  1848    22    23   125\n",
      "     71    11     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0]\n",
      " [   11    19     6    35  5607     9   301     1 12549    43     4     1\n",
      "   5607     1   860     6  1752     1     4     1   111     6  1752     1\n",
      "    351    23    30   115  7133    14     6     1   455    57     1   153\n",
      "    303   143   128  4559   543    15   146    11    19  3071     2 14443\n",
      "     23  2396    16  3952  4675  1733  7489     6  4339    18   634   560\n",
      "     12     5     1  2408   526     5  1463  1312  1005    37  7723     2\n",
      "   8404    23  3744     5  8798    30    32  8882   648    10 17168    45\n",
      "     11   931    96    73    37     3  8483    18    15   291    34  3628\n",
      "     70   117  1116   105     3  1266    10    25   112  5155   543     5\n",
      "    230    11  2400     5     1  5187     8   117  2473   150     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5, 170)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# word embeddings of sample data\n",
    "sample_embed = tokenizer.texts_to_sequences(sample_data)\n",
    "sample_embed_padded = K.preprocessing.sequence.pad_sequences(sample_embed, maxlen=maxlen, padding=\"post\")\n",
    "\n",
    "print(sample_embed)\n",
    "print(sample_embed_padded)\n",
    "sample_embed_padded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load saved keras model in h5\n",
    "from keras.models import load_model\n",
    "loaded_model = load_model('imdb-lstm-model7.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_layer (Embedding)      (None, None, 128)         2560000   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "output_layer (Dense)         (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 2,691,713\n",
      "Trainable params: 2,691,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "loaded_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = loaded_model.predict(sample_embed_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 1)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert  is an amazing actor and now the same being director  father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for  and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also  to the two little boy's that played the  of norman and paul they were just brilliant children are often left out of the  list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\n",
      "Positive probability:  0.00804227 \n",
      "\n",
      "To put this in context, I am 34 years old and I have to say that this is the best film I have seen without doubt and I don't expect it will be beaten as far as I am concerned. Obviously times move on, and I acknowledge that due to its violence and one particularly uncomfortable scene this film is not for everyone, but I still remember watching it for the first time, and it blew me away. Anyone who watches it now has to remember that it actually changed the history of cinema. In context- it followed a decade or more of action films that always ended with a chase sequence where the hero saved the day - you could have written those films yourself. Pulp had you gripped and credited the audience with intelligence. There is not a line of wasted dialogue and the movie incorporates a number of complexities that are not immediately obvious. It also resurrected the career of Grease icon John Travolta and highlighted the acting talent of Samuel L Jackson. There are many films now that are edited out of sequence and have multiple plots etc but this is the one they all want to be, or all want to beat, but never will.\n",
      "Positive probability:  0.98287946 \n",
      "\n",
      "Viewers are taken on a ride through three different stories that entertwine together around the world of Marcellus Wallace. Quentin Tarantino proves that he is the master of witty dialogue and a fast plot that doesn't allow the viewer a moment of boredom or rest. From the story of two hit-man on a job, to a fixed boxing match to a date between a hit-man and the wife of a mob boss. There was definitely a lot of care into the writing of the script, as everything no matter the order it is in, fits with the story. Many mysteries have been left such as what is inside of the briefcase and why Marcellus Wallace has a band-aid on the back of his neck, which may be connected. The movie redefined the action genre and reinvigorated the careers of both John Travolta and Bruce Willis. This movie is required viewing for any fan of film.\n",
      "Positive probability:  0.021172293 \n",
      "\n",
      "Was it the money? Did he owe someone a favour? Cage why are you in this terrible movie ? Left Behind is even worse than the Kirk Cameron version ,which also sucked. The book I am sure is better, since many have read it. This movie is a career killer ...it is sad to see Cage left behind by Hollywood. The acting is bad , Cage tries hard to make work out of a script that was seemingly penned by 8 year old's . I mean the dialogue is terrible. Why waste your money making a film that is just bad. Cage needs to really rethink his career . Honestly I felt ashamed for Nick . Wild at Heart , Leaving Las Vegas, and Oscar winner... My God man have you no self respect? Nick you are better than this.\n",
      "Positive probability:  0.47735694 \n",
      "\n",
      "This film is so banal it takes the banality out of The Banal. The premise is absurd. The unravelling of the plot is absurd. The performances are, at best, distracted as is the direction. Even the actors seem unconvinced. I'm still kicking myself for watching this film. Sin and purity are depicted with juvenile simplicity. Religious bias is arrogantly displayed (but I'll leave that to the unfortunate viewer to spot). Complex ideas like forgiveness, atonement and judgement are reduced to idiocy at an atomic level. I apologise if this sounds too much like a rant but for someone who watches well over 15 films a week, I have never subjected myself to anything this offensive (to the senses) in over 25 years.\n",
      "Positive probability:  0.5218705 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(sample_data)):\n",
    "    prob = predictions[i]\n",
    "    print(sample_data[i])\n",
    "    print('Positive probability: ', prob[0], '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
